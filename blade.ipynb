{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZoliN/colab/blob/main/blade.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJEFdQ_FgChu",
        "outputId": "9a680b49-b72e-4e5a-97d9-7a1c7ff0c11c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqsfE7V8DfvS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIKAVeuFGdJ7"
      },
      "outputs": [],
      "source": [
        "!unzip \"/content/drive/MyDrive/Colab Notebooks/zip.zip\" -d /content/\n",
        "%cd /content/zip\n",
        "!mkdir build\n",
        "%cd build\n",
        "!cmake .. -DCMAKE_BUILD_TYPE=Release"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKO7Gs9TazN6"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!wget  http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip\n",
        "!mkdir pics\n",
        "!unzip DIV2K_train_HR.zip -d pics/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qs-jiTJ9dtk5"
      },
      "outputs": [],
      "source": [
        "%cd /content/pics/DIV2K_train_HR\n",
        "!sh /content/drive/MyDrive/blade/copypics.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGU_PynQMCBJ"
      },
      "outputs": [],
      "source": [
        " %cd /content/zip/build\n",
        "!make -j4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVPCGRqEAB-K"
      },
      "outputs": [],
      "source": [
        "!cp \"/content/drive/MyDrive/Colab Notebooks/2023_5_28_1_35_27.filter\"  /content/zip/filters/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0UONCsDbi04"
      },
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0UXLfCNOFyj",
        "outputId": "d73ab8f3-2f11-4c54-e7d2-3daf93f82b10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/zip/train_images\n"
          ]
        }
      ],
      "source": [
        "%cd /content/zip/train_images\n",
        "!mv *.png ./p/\n",
        "!mv ./p/08.png .\n",
        "!for i in {1..1000}; do cp 08.png \"test$i.png\"; done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWc4SrbPwJyB"
      },
      "outputs": [],
      "source": [
        "%cd /content/zip/\n",
        "!./build/blade\n",
        "!cp -r filters /content/drive/MyDrive/blade/\n",
        "!cp -r result_images /content/drive/MyDrive/blade/\n",
        "!cp temp_images/*.csv /content/drive/MyDrive/blade/\n",
        "while True:pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcweFQIsl_Sb"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/temp_images\n",
        "!mkdir /content/result_images\n",
        "!mkdir /content/filters\n",
        "!mkdir /content/test_images\n",
        "%cd /content/\n",
        "!/content/build/blade\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!du -h /content/drive/MyDrive/superrestf/bladesr/temp_images/*"
      ],
      "metadata": {
        "id": "dalu1XD_GHLL",
        "outputId": "c005a043-9b8b-4f65-c944-483a2a49ea6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "du: cannot access '/content/drive/MyDrive/superrestf/bladesr/temp_images/*': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmzc68Ot5ra9"
      },
      "outputs": [],
      "source": [
        "!zip -r /content/drive/MyDrive/bladearch/src.zip /content/drive/MyDrive/blade/*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "!mkdir build\n",
        "%cd build\n",
        "!cmake /content/drive/MyDrive/superrestf/bladesr -DCMAKE_BUILD_TYPE=Release\n",
        "!pip install python_magic\n",
        "!pip install tensorflow_addons\n",
        "!apt-get install libmagic1\n",
        "%cd /content/\n",
        "!echo 12.png > filelist.txt\n",
        "!echo 12.png > filelistv.txt"
      ],
      "metadata": {
        "id": "kUKzkXvvV7ZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " %cd /content/build\n",
        "!make -j4"
      ],
      "metadata": {
        "id": "rvKf-9a3WOxs",
        "outputId": "a2ee210e-2da0-4472-e620-022c206a3181",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/build\n",
            "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/blade.dir/main.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32m\u001b[1mLinking CXX executable blade\u001b[0m\n",
            "[100%] Built target blade\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm /content/input\n",
        "!ln -sf /content/drive/MyDrive/superrestf/bladesr/test_images /content/input\n",
        "#!ln -sf /content/pics/DIV2K_train_HR /content/input\n",
        "#!cp /content/drive/MyDrive/superrestf/bladesr/test_images/* /content/input/\n",
        "\n",
        "import os\n",
        "os.makedirs('/content/anal_images', exist_ok=True)\n",
        "!/content/build/blade\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def iterate_files(directory):\n",
        "    for file in os.listdir(directory):\n",
        "        yield os.path.join(directory, file)\n",
        "\n",
        "inpath = '/content/anal_images'\n",
        "outpath = '/content/anal_tensors'\n",
        "os.makedirs(outpath, exist_ok=True)\n",
        "# Iterate over the files of the current directory\n",
        "for file in iterate_files(inpath):\n",
        "    filename = os.path.basename(file)\n",
        "    print(filename)\n",
        "    im_input = cv2.imread(file, -1)\n",
        "    print(im_input.shape)\n",
        "    if filename == '12.png.tiff':\n",
        "      np.savetxt('/content/c2.txt', im_input[:,:,3])\n",
        "\n",
        "    byte_string = tf.io.serialize_tensor(im_input)\n",
        "    # Write the byte string to a file\n",
        "    with tf.io.gfile.GFile(outpath + '/' + filename[:-5] + '.bin', 'wb') as f:\n",
        "        f.write(byte_string.numpy())\n"
      ],
      "metadata": {
        "id": "q_XyDd3LWiJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import numpy as np\n",
        "from importlib import reload\n",
        "import superrestf.models2 as models\n",
        "from importlib import reload\n",
        "reload(models)\n",
        "import superrestf.data_pipeline as dp\n",
        "reload(dp)\n",
        "\n",
        "scale = 2\n",
        "scalef = float(scale)\n",
        "invscale = 1./scalef\n",
        "\n",
        "img_raw = tf.io.read_file('/content/drive/MyDrive/superrestf/bladesr/test_images/12.png')\n",
        "im_input = tf.image.decode_image(img_raw)\n",
        "im_input = tf.cast(im_input, dtype=tf.float32)/255.0\n",
        "\n",
        "# img_raw = tf.io.read_file('/content/result_images/BLADE_12.png')\n",
        "# im_input2 = tf.image.decode_image(img_raw, channels=1)\n",
        "# im_input2 = tf.cast(im_input2, dtype=tf.float32)/255.0\n",
        "# print('PSNR: ' + str(tf.image.psnr(im_input2[8:512-8,8:512-8,:], im_input[8:512-8,8:512-8,:], max_val=1.0).numpy()))\n",
        "\n",
        "orig_sz = tf.shape(im_input)[0:2]\n",
        "lr_sz = orig_sz // scale\n",
        "full_sz = lr_sz * scale\n",
        "\n",
        "\n",
        "analpath = '/content/anal_tensors'\n",
        "byte_string = []\n",
        "with tf.io.gfile.GFile(analpath + '/' + '12.png' + '.bin', 'rb') as f:\n",
        "    byte_string = f.read()\n",
        "\n",
        "# Deserialize the tensor from the byte string\n",
        "anal_tensor = tf.io.parse_tensor(byte_string, out_type=tf.float32)\n",
        "print(tf.shape(input=anal_tensor))\n",
        "\n",
        "lam1, lam2, c, s = tf.unstack(anal_tensor, axis=-1)\n",
        "#np.savetxt('/content/c.txt', c.numpy())\n",
        "#np.savetxt('/content/s.txt', s.numpy())\n",
        "# np.savetxt('/content/lam1.txt', lam1.numpy())\n",
        "# np.savetxt('/content/lam2.txt', lam2.numpy())\n",
        "Dtr = 300. * 1.525902E-05 * 0.860704768\n",
        "Dth = 100. * 1.525902E-05 * 0.860704768 * 1. / Dtr;\n",
        "kDetail = 0.1;\n",
        "kDenoise = 0.15;\n",
        "kStretch = 4.;\n",
        "kShrink = 1.;\n",
        "\n",
        "# Dtr=0.0039259493\n",
        "# Dth=0.33213648\n",
        "# kDetail=0.6002224\n",
        "# kDenoise=0.5469237\n",
        "# kStretch=2.291745\n",
        "# kShrink=2.3721561\n",
        "\n",
        "# Dtr=0.48\n",
        "# Dth=0.801\n",
        "# kDetail=0.1388774\n",
        "# kDenoise=0.272592\n",
        "# kStretch=3.6042476\n",
        "# kShrink=0.590034\n",
        "\n",
        "kDetail2 = 0.4\n",
        "kDenoise2 = 0.8\n",
        "\n",
        "\n",
        "A = (lam1 - lam2) / (lam1 + lam2 + 0.0000000001);\n",
        "A = tf.clip_by_value(A, 0.1, 1.0);\n",
        "\n",
        "D = 1. - tf.sqrt(lam1) / Dtr + Dth\n",
        "D = tf.clip_by_value(D, 0.0, 1.0)\n",
        "\n",
        "k1h = kDetail * (kStretch * A +  (1. - A));\n",
        "k2h = kDetail / (kShrink * A +  (1. - A));\n",
        "\n",
        "k1 = ((1.0 - D)*k1h + D*kDenoise);\n",
        "k2 = ((1.0 - D)*k2h + D*kDenoise);\n",
        "k1 *= k1;\n",
        "k2 *= k2;\n",
        "\n",
        "x2 = c;\n",
        "y2 = s;\n",
        "x1 = s;\n",
        "y1 = -c;\n",
        "\n",
        "b11 = k1*x1*x1 + x2*x2*k2;\n",
        "b12 = k1*x1*y1 + x2*y2*k2;\n",
        "b22 = k1*y1*y1 + y2*y2*k2;\n",
        "\n",
        "det = b11*b22 - b12*b12 + 0.0000000001;\n",
        "\n",
        "m11 = b22 / det;\n",
        "m22 = b11 / det;\n",
        "m12 = -b12 / det;\n",
        "\n",
        "kerp = tf.stack((m11, m22, m12), axis = -1)\n",
        "kerp = tf.image.resize(kerp, full_sz, method='bilinear')\n",
        "m11, m22, m12 = tf.split(kerp, 3, axis = -1)\n",
        "\n",
        "#np.savetxt('/content/s.txt', s.numpy())\n",
        "kerp = tf.stack((s, c, A), axis = -1)\n",
        "kerp = tf.image.resize(kerp, full_sz, method='area')\n",
        "cos, sin, Coh = tf.split(kerp, 3, axis = -1)\n",
        "\n",
        "\n",
        "params = {}\n",
        "params['net_input_size'] = 512\n",
        "params['numimgs'] = 8\n",
        "\n",
        "\n",
        "numimgs = 8\n",
        "shifts = tf.random.uniform(shape=[numimgs, 2], minval=-0.99, maxval=0.99, dtype=tf.float32, seed=4567)\n",
        "\n",
        "im_lst = [dp.shiftImg(im_input, shifts[i,:],params['net_input_size'] ) for i in range(numimgs)]\n",
        "im_all = tf.concat(im_lst, 2)\n",
        "\n",
        "shifts += tf.random.uniform(shape=[numimgs, 2], minval=-0.15, maxval=0.15, dtype=tf.float32, seed=1234)\n",
        "shifts = tf.clip_by_value(shifts, -0.99, 0.99)\n",
        "print('shifts: ')\n",
        "print(shifts)\n",
        "\n",
        "#im_lr = tf.image.resize(im_input, lr_sz, method='area')\n",
        "im_lr = tf.image.resize(im_all, lr_sz, method='area')\n",
        "\n",
        "im_hr = tf.image.resize(im_lr, full_sz, method='nearest')\n",
        "\n",
        "lam1, lam2, c, s = tf.split(anal_tensor, 4, axis=-1)\n",
        "#c = tf.fill(tf.shape(c), 1.)\n",
        "anal_tensor = tf.concat( (lam1, lam2, c, s), 2)\n",
        "\n",
        "#mask = tf.where( (tf.sqrt(lam1) - 12.0 * noiseCorr) / (noiseCorr * 10.0) > 0.0, im_lr, 0.0)\n",
        "A = tf.expand_dims(A, 2)\n",
        "mask =  tf.where( tf.logical_and(tf.sqrt(lam1) > 0.1, A > 0.9), 1.0,\n",
        "           tf.where(tf.logical_and(tf.sqrt(lam1) > 0.05, A > 0.75), 0.5, 0.0))\n",
        "mask = tf.image.resize(mask, full_sz, method='nearest')\n",
        "print(tf.shape(mask))\n",
        "cv2.imwrite('/content/mask.png', mask.numpy()*255.0)\n",
        "cv2.imwrite('/content/stre.png', tf.clip_by_value(tf.sqrt(lam1), 0., 1.).numpy()*255.0)\n",
        "\n",
        "checkpoint_filepath = '/content/checkpoint'\n",
        "model = models.SRBlade(params)\n",
        "try:\n",
        "  model.load_weights(checkpoint_filepath)\n",
        "except:\n",
        "  print(\"Couldn't load weights from \",checkpoint_filepath)\n",
        "\n",
        "model.params['net_input_size'] = tf.Variable(initial_value = 512, trainable=False, name= 'params_net_input_size')\n",
        "\n",
        "model.compile(loss=['mse', returnPrediction], metrics={'output_1':PSNR},\n",
        "  run_eagerly=True)\n",
        "\n",
        "\n",
        "\n",
        "outimg = model( {'image_input' : tf.expand_dims(im_lr,0), 'anal_input' : tf.expand_dims(anal_tensor,0), 'mask_input' : tf.expand_dims(mask, 0), 'shifts': tf.expand_dims(shifts, 0)})\n",
        "print(model.trainable_variables)\n",
        "\n",
        "print('PSNR mmodel: ' + str(tf.image.psnr(tf.squeeze(outimg ,0), im_input , max_val=1.0).numpy()))\n",
        "cv2.imwrite('/content/outm.png', tf.squeeze(outimg,0).numpy()*255.0)\n",
        "\n",
        "\n",
        "# cv2.imwrite('/content/im_lr.png', im_lr.numpy()*255.0)\n",
        "# cv2.imwrite('/content/im_hr.png', im_hr.numpy()*255.0)\n",
        "\n",
        "# im_hr2 = tf.image.resize(im_lr, full_sz, method='lanczos3')\n",
        "# cv2.imwrite('/content/im_hr2.png', im_hr2.numpy()*255.0)\n",
        "\n",
        "\n",
        "paddings = tf.constant([[2 * scale + 1, 2 * scale + 1], [2 * scale + 1, 2 * scale + 1], [0, 0]])\n",
        "# paddings = tf.constant([[2, 2], [2, 2], [0, 0]])\n",
        "im_padded = tf.pad(im_hr, paddings, \"SYMMETRIC\")\n",
        "im_paddedlst = []\n",
        "for i in range(numimgs):\n",
        "  xshift = 2 if shifts[i,0] < -0.5 else 0 if shifts[i,0] >= 0.5 else 1\n",
        "  yshift = 2 if shifts[i,1] < -0.5 else 0 if shifts[i,1] >= 0.5 else 1\n",
        "  im_paddedlst.append(im_padded[yshift:yshift+full_sz[0] + 4 * scale, xshift: xshift+full_sz[1] + 4 * scale, i:i+1])\n",
        "#im_padded = tf.stack(im_paddedlst, 0)\n",
        "im_padded = tf.concat(im_paddedlst, -1)\n",
        "\n",
        "#tf.convert_to_tensor(value, dtype=None\n",
        "#im_dst = tf.fill(tf.concat( full_sz, tf.shape(im_lr)[2]), 0.0)\n",
        "\n",
        "#im_dst = tf.fill(tf.shape(im_padded[:,:full_sz[0],:full_sz[1],: ]), 0.0)\n",
        "#im_w = tf.fill(tf.shape(im_padded[:,:full_sz[0],:full_sz[1],:]), 0.0)\n",
        "im_dst = tf.fill(tf.shape(im_hr), 0.0)\n",
        "im_w = tf.fill(tf.shape(im_hr), 0.0)\n",
        "\n",
        "\n",
        "print(tf.shape(input=im_dst))\n",
        "print(tf.shape(input=im_padded))\n",
        "\n",
        "\n",
        "# Create two 1D arrays\n",
        "x_coords = np.arange(float(full_sz[0]))\n",
        "y_coords = np.arange(float(full_sz[1]))\n",
        "\n",
        "# Create a 2D NumPy array containing x and y indices\n",
        "indices = np.meshgrid(x_coords, y_coords)\n",
        "\n",
        "coords = np.stack([indices[0], indices[1]], 2)\n",
        "\n",
        "#cv2.imwrite('/content/im_' + str(i) + '.png',  im_hr[:, :, i:i+1].numpy()*255.0)\n",
        "\n",
        "\n",
        "srcpos1 = (coords + 0.4995) * invscale\n",
        "srcpos = tf.convert_to_tensor(srcpos1, dtype=tf.float32)\n",
        "srcpos = tf.expand_dims(srcpos, 3)\n",
        "shifts = tf.expand_dims(tf.transpose(shifts, perm = [1,0]), 0)\n",
        "#shifts = tf.expand_dims(shifts, 1)\n",
        "shifts = tf.expand_dims(shifts, 0)\n",
        "srcpos -= shifts * invscale\n",
        "srcMidPos = tf.floor(srcpos) + 0.5;\n",
        "\n",
        "\n",
        "ls = 3.\n",
        "xt = 0.5\n",
        "pi = 3.14159265\n",
        "for y in range(5):\n",
        "  ys = y * scale\n",
        "  y1 = float(y - 2)\n",
        "  for x in range(5):\n",
        "    xs = x * scale\n",
        "    x1 = float(x - 2)\n",
        "    posk = (srcMidPos + tf.expand_dims((x1, y1),1) - srcpos) #* scalef\n",
        "    #poskx, posky = tf.split(posk, 2, axis = -2)\n",
        "    poskx, posky = tf.unstack(posk, axis = -2)\n",
        "    # poskxt = (cos * poskx - sin * posky) * xt\n",
        "    # poskyt = sin * poskx + cos * posky\n",
        "    # poskx = poskxt\n",
        "    # posky = poskyt\n",
        "    #if (x==2 and y==2):\n",
        "    #  print(poskx.numpy()[0:4,0:4])\n",
        "    #kwx = tf.where(poskx == 0.0, 1.0, ls * tf.sin(pi * poskx) * tf.sin(pi * poskx / ls) / (pi * pi * poskx * poskx))\n",
        "    #kwy = tf.where(posky == 0.0, 1.0, ls * tf.sin(pi * posky) * tf.sin(pi * posky / ls) / (pi * pi * posky * posky))\n",
        "    #kw = kwx * kwy\n",
        "    kw = tf.exp(-0.5 * ((poskx*m11+posky*m12)*poskx+(poskx*m12+posky*m22)*posky) )\n",
        "    #kw2 = tf.exp(-0.5 * ((poskx*m11_2+posky*m12_2)*poskx+(poskx*m12_2+posky*m22_2)*posky) )\n",
        "    #kw = kw1 + 10. * kw2\n",
        "    # kw = kw1 * 2.0 - kw2 * 1.0\n",
        "    #if (x==4 and y==2):\n",
        "    #  print(kw.numpy()[0:4,0:4])\n",
        "    #if x == 2 and y == 2:\n",
        "    im_w += kw\n",
        "    im_dst+= kw * im_padded[ys:ys+full_sz[0], xs: xs+full_sz[1], :]\n",
        "    #im_dst+= kw * im_padded[ys+yshift:ys+yshift+full_sz[0], xs+xshift: xs+xshift+full_sz[1], i:i+1]\n",
        "\n",
        "\n",
        "# halfRatio = invscale * 0.5\n",
        "# for y in range(5):\n",
        "#   y1 = float(y - 2)\n",
        "#   for x in range(5):\n",
        "#     x1 = float(x - 2)\n",
        "#     srcposc = srcpos + (x1 * invscale, y1 * invscale)\n",
        "#     comp = tf.where(tf.abs(srcposc - tf.floor(srcposc) - 0.5) < halfRatio, 1.0, 0.0)\n",
        "#     compx, compy = tf.split(comp, 2, axis = -1)\n",
        "#     kw = tf.where(compx * compx == 1.0, tf.exp(-0.5 * ((x1*m11+y1*m12)*x1+(x1*m12+y1*m22)*y1) ), 0.0)\n",
        "#     #if (x==3 and y==2):\n",
        "#     #  print(kw.numpy()[200:204,200:204])\n",
        "#     im_w += kw\n",
        "#     im_dst+= kw * im_padded[y:y+full_sz[0], x: x+full_sz[1]]\n",
        "\n",
        "im_dst = tf.reduce_sum(im_dst, -1, True)\n",
        "im_w = tf.reduce_sum(im_w, -1, True)\n",
        "im_dst = im_dst / (im_w + 0.0000001)\n",
        "\n",
        "cv2.imwrite('/content/out.png', im_dst.numpy()*255.0)\n",
        "\n",
        "cv2.imwrite('/content/test.jpg', A.numpy()*255.0)\n",
        "\n",
        "print(tf.shape(input=D))\n",
        "print('PSNR: ' + str(tf.image.psnr(im_dst, im_input, max_val=1.0).numpy()))\n",
        "# print('PSNR upsampl: ' + str(tf.image.psnr(im_hr2, im_input, max_val=1.0).numpy()))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZDssoEtqQQWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm /content/input/DIV2K_train_HR\n",
        "!rm /content/input/test_images\n",
        "!rm /content/input\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ykio78zrkina",
        "outputId": "9fb29627-a7c1-4d1b-9657-45b46f416060"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/input/DIV2K_train_HR': No such file or directory\n",
            "rm: cannot remove '/content/input/test_images': No such file or directory\n",
            "rm: cannot remove '/content/input': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm /content/input/DIV2K_train_HR\n",
        "!rm /content/input\n",
        "!ln -sf /content/drive/MyDrive/superrestf/bladesr/test_images /content/input\n",
        "#!ln -sf /content/pics/DIV2K_train_HR /content/input\n",
        "#!cp /content/drive/MyDrive/superrestf/bladesr/test_images/* /content/input/\n",
        "!rm /content/checkpoint*\n",
        "%cd /content/drive/MyDrive/\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "import skimage\n",
        "import skimage.io\n",
        "import logging\n",
        "from keras.models import load_model\n",
        "from tensorflow.python.ops.math_ops import truediv\n",
        "import tensorflow_probability as tfp\n",
        "#import tensorflow_addons as tfa\n",
        "\n",
        "\n",
        "import superrestf.models2 as models\n",
        "import superrestf.data_pipeline as dp\n",
        "\n",
        "from importlib import reload\n",
        "\n",
        "reload(models)\n",
        "reload(dp)\n",
        "\n",
        "\n",
        "numImgChs = 1\n",
        "\n",
        "logging.basicConfig(format=\"[%(process)d] %(levelname)s %(filename)s:%(lineno)s | %(message)s\")\n",
        "log = logging.getLogger(\"train\")\n",
        "log.setLevel(logging.INFO)\n",
        "\n",
        "def PSNR(im1, im2):\n",
        "  return tf.image.psnr(im1, im2, max_val=1.0)\n",
        "\n",
        "def returnPrediction(target, pred):\n",
        "  return pred\n",
        "\n",
        "def return0(target, pred):\n",
        "  return 0\n",
        "\n",
        "\n",
        "def l2_lossHist(target, prediction):\n",
        "  loss = tf.reduce_mean(input_tensor=tf.square(target-prediction))\n",
        "\n",
        "  percs = np.linspace(1, 100, 64, endpoint=False, dtype=np.float32)\n",
        "  hist1 = tfp.stats.percentile(target, percs, [1,2,3])\n",
        "  hist2 = tfp.stats.percentile(prediction, percs, [1,2,3])\n",
        "  histdiff2 = tf.reduce_mean(input_tensor=tf.square(hist1-hist2))\n",
        "  return loss + histdiff2 * 10.\n",
        "\n",
        "def PSNR_w(target, prediction):\n",
        "  rgb, segmented = tf.split(target, [numImgChs,1], 3)\n",
        "  squares = segmented * tf.square(rgb-prediction)\n",
        "  squares = tf.reshape(squares, [tf.shape(input=squares)[0], -1])\n",
        "  p = (-10/np.log(10))*tf.math.log(tf.reduce_sum(input_tensor=squares, axis=[1])\n",
        "          / tf.reduce_sum(input_tensor=segmented, axis=[1,2,3]))\n",
        "  return p\n",
        "\n",
        "\n",
        "def l2_w(target, prediction):\n",
        "  rgb, segmented = tf.split(target, [numImgChs,1], 3)\n",
        "  squares = segmented * tf.square(rgb-prediction)\n",
        "  l2 = tf.reduce_sum(input_tensor=squares) / tf.reduce_sum(input_tensor=segmented)\n",
        "  return l2\n",
        "\n",
        "params = {}\n",
        "params['net_input_size'] = 256\n",
        "params['numimgs'] = 8\n",
        "\n",
        "checkpoint_filepath = '/content/checkpoint'\n",
        "data_dir = '/content/filelist.txt'\n",
        "validation_dir = '/content/filelistv.txt'\n",
        "#validation_dir = ''\n",
        "output_resolution = [378, 504]\n",
        "output_resolution = [384, 510]\n",
        "# output_resolution = [1512, 2016]\n",
        "fliplr=False\n",
        "flipud=False\n",
        "rotate=False\n",
        "random_crop=True\n",
        "shuffle=False\n",
        "batch_size = 1\n",
        "learning_rate = 1e-3\n",
        "\n",
        "net_input_size = params['net_input_size']\n",
        "\n",
        "train_samples = dp.ImageFilesDataPipeline(\n",
        "    data_dir,\n",
        "    shuffle=shuffle,\n",
        "    net_input_size=params['net_input_size'],\n",
        "    capacity=1,\n",
        "    min_after_dequeue = 4,\n",
        "    batch_size=batch_size,\n",
        "    fliplr=fliplr, flipud=flipud, rotate=rotate,\n",
        "    random_crop=random_crop,\n",
        "    custaugment=False,\n",
        "    exposuremult=False,\n",
        "    output_resolution=output_resolution,\n",
        "    keras = True,\n",
        "    repeat = True,\n",
        "    numimgs = params['numimgs']).samples\n",
        "\n",
        "validation_samples = None\n",
        "if validation_dir != '':\n",
        "  validation_samples = dp.ImageFilesDataPipeline(\n",
        "      validation_dir,\n",
        "      net_input_size=params['net_input_size'],\n",
        "      capacity=1,\n",
        "      batch_size=1,\n",
        "      fliplr=False,flipud=False, rotate=False,\n",
        "      random_crop=True,\n",
        "      exposuremult=False,\n",
        "      output_resolution=output_resolution,\n",
        "      keras = True,\n",
        "      repeat = False,\n",
        "      numimgs = params['numimgs']).samples\n",
        "\n",
        "\n",
        "opti=tf.keras.optimizers.Adam(learning_rate, use_ema=False, ema_momentum=0.1, weight_decay=0.001)\n",
        "#opti=tfa.optimizers.MovingAverage(opti, 0.99)\n",
        "model = models.SRBlade(params)\n",
        "#model = models.HDRNet(params)\n",
        "\n",
        "#model.run_eagerly = False\n",
        "#model = load_model('/content/hdrnet/train/out', compile=False)\n",
        "try:\n",
        "  model.load_weights(checkpoint_filepath)\n",
        "except:\n",
        "  print(\"Couldn't load weights from \",checkpoint_filepath)\n",
        "\n",
        "model.compile(optimizer=opti, loss=['mse', returnPrediction], metrics={'output_1':PSNR},\n",
        "  run_eagerly=False)\n",
        "# model.compile(optimizer=opti, loss=[l2_w, returnPrediction], metrics={'output_1':PSNR_w, 'output_2':return0},\n",
        "# run_eagerly=False)\n",
        "\n",
        "\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "#model_checkpoint_callback = tfa.callbacks.AverageModelCheckpoint(\n",
        "  update_weights=True,\n",
        "  filepath=checkpoint_filepath,\n",
        "  save_weights_only=True,\n",
        "  monitor= ('val_' if validation_dir != '' else '') + 'PSNR',\n",
        "  #monitor= 'output_1_PSNR',\n",
        "  mode='max',\n",
        "  verbose=1,\n",
        "  save_freq = 'epoch',\n",
        "  save_best_only=False)\n",
        "\n",
        "\n",
        "\n",
        "history = model.fit(train_samples, epochs=50, steps_per_epoch = 400//batch_size, callbacks=[model_checkpoint_callback],\n",
        "  validation_data = validation_samples, validation_freq = 1, verbose = 'auto')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wIShtZgraOMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(model.Dtr)\n",
        "print(model.Dth)\n",
        "\n",
        "print(model.kDetail)\n",
        "print(model.kDenoise)\n",
        "print(model.kStretch)\n",
        "print(model.kShrink)\n",
        "\n",
        "print(model.kDetail2)\n",
        "print(model.kDenoise2)\n",
        "print(model.kStretch2)\n",
        "print(model.kShrink2)\n",
        "\n",
        "print(3000. * 1.525902E-05 * 0.860704768)\n",
        "print(100. * 1.525902E-05 * 0.860704768 / (3000. * 1.525902E-05 * 0.860704768))"
      ],
      "metadata": {
        "id": "xu3ZRJ04VY2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def iterate_files(directory):\n",
        "    for file in os.listdir(directory):\n",
        "        yield os.path.join(directory, file)\n",
        "\n",
        "inpath = '/content/anal_tensors'\n",
        "# Iterate over the files of the current directory\n",
        "for file in iterate_files(inpath):\n",
        "  filename = os.path.basename(file)\n",
        "  byte_string = []\n",
        "  with tf.io.gfile.GFile(file, 'rb') as f:\n",
        "      byte_string = f.read()\n",
        "\n",
        "# Deserialize the tensor from the byte string\n",
        "  anal_tensor = tf.io.parse_tensor(byte_string, out_type=tf.float32)\n",
        "\n",
        "  lam1, lam2, c, s = tf.split(anal_tensor, 4, axis=-1)\n",
        "\n",
        "  A = (lam1 - lam2) / (lam1 + lam2 + 0.0000000001);\n",
        "  A = tf.clip_by_value(A, 0.1, 1.0)\n",
        "  mask = tf.where( tf.logical_and(tf.sqrt(lam1) > 0.1, A > 0.9), 1.0, 0.0)\n",
        "  avg = tf.reduce_mean(mask)\n",
        "  if avg.numpy() < 0.01:\n",
        "    print(filename + \" : \"  + str(avg.numpy()))\n",
        "  #mask = tf.image.resize(mask, full_sz, method='nearest')\n"
      ],
      "metadata": {
        "id": "FFQsNHmxiCvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ln -s /content/drive/MyDrive/superrestf/bladesr/test_images /content/input\n",
        "%cd /content/drive/MyDrive/superrestf/bin\n",
        "!python train2.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWdloDf7Gova",
        "outputId": "3c1c8ce8-e65d-4241-feb3-db6637429785"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!zip -r /content/drive/MyDrive/bladearch/result.zip /content/result_images/*\n",
        "!unzip /content/drive/MyDrive/bladearch/result.zip -d /\n",
        "!cp /content/drive/MyDrive/blade/filters/2024_2_18_0_12_59.filter /content/filters/"
      ],
      "metadata": {
        "id": "cVLOyQGumlCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "img_raw = tf.io.read_file('/content/drive/MyDrive/superrestf/bladesr/test_images/12.png')\n",
        "im_input = tf.image.decode_image(img_raw)\n",
        "im_input = tf.cast(im_input, dtype=tf.float32)/255.0\n",
        "\n",
        "full_sz = tf.shape(im_input)[0:2]\n",
        "scale = 1\n",
        "scalef = float(scale)\n",
        "invscale = 1./scalef\n",
        "\n",
        "paddings = tf.constant([[2 * scale, 2 * scale], [2 * scale, 2 * scale], [0, 0]])\n",
        "# paddings = tf.constant([[2, 2], [2, 2], [0, 0]])\n",
        "im_padded = tf.pad(im_input, paddings, \"SYMMETRIC\")\n",
        "\n",
        "im_dst = tf.fill(tf.shape(im_input), 0.0)\n",
        "im_w = tf.fill(tf.shape(im_input), 0.0)\n",
        "\n",
        "# Create two 1D arrays\n",
        "x_coords = np.arange(float(full_sz[0]))\n",
        "y_coords = np.arange(float(full_sz[1]))\n",
        "\n",
        "# Create a 2D NumPy array containing x and y indices\n",
        "indices = np.meshgrid(x_coords, y_coords)\n",
        "\n",
        "srcpos1 = np.stack([indices[0], indices[1]], 2)\n",
        "srcpos1 = (srcpos1 + 0.4995) * invscale + 0.2\n",
        "srcpos = tf.convert_to_tensor(srcpos1, dtype=tf.float32)\n",
        "srcMidPos = tf.floor(srcpos) + 0.5;\n",
        "\n",
        "ls = 3.\n",
        "pi = 3.14159265\n",
        "for y in range(5):\n",
        "  ys = y * scale\n",
        "  y1 = float(y - 2)\n",
        "  for x in range(5):\n",
        "    xs = x * scale\n",
        "    x1 = float(x - 2)\n",
        "    posk = (srcMidPos + (x1, y1) - srcpos) #* scalef\n",
        "    poskx, posky = tf.split(posk, 2, axis = -1)\n",
        "\n",
        "    kwx = tf.where(poskx == 0.0, 1.0, ls * tf.sin(pi * poskx) * tf.sin(pi * poskx / ls) / (pi * pi * poskx * poskx))\n",
        "    kwy = tf.where(posky == 0.0, 1.0,ls * tf.sin(pi * posky) * tf.sin(pi * posky / ls) / (pi * pi * posky * posky))\n",
        "    kw = kwx * kwy\n",
        "\n",
        "    im_w += kw\n",
        "    im_dst+= kw * im_padded[ys:ys+full_sz[0], xs: xs+full_sz[1]]\n",
        "\n",
        "\n",
        "\n",
        "im_dst = im_dst / (im_w + 0.0000001)\n",
        "\n",
        "cv2.imwrite('/content/imout.png', im_dst.numpy()*255.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxtdIuA7d7SN",
        "outputId": "186fb5fa-3808-4360-e011-3d87ba987171"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import numpy as np\n",
        "from importlib import reload\n",
        "import superrestf.models2 as models\n",
        "from importlib import reload\n",
        "\n",
        "\n",
        "img_raw = tf.io.read_file('/content/drive/MyDrive/superrestf/bladesr/test_images/12.png')\n",
        "im_input = tf.image.decode_image(img_raw)\n",
        "im_input = tf.cast(im_input, dtype=tf.float32)/255.0\n",
        "\n",
        "img_raw = tf.io.read_file('/content/result_images/BLADE_12.png')\n",
        "im_input2 = tf.image.decode_image(img_raw, channels=1)\n",
        "im_input2 = cv2.bilateralFilter(im_input2.numpy(), 3, 20, 1000)\n",
        "im_input2 = tf.cast(im_input2, dtype=tf.float32)/255.0\n",
        "im_input2 = tf.expand_dims(im_input2, 2)\n",
        "shape = im_input2.get_shape().as_list()\n",
        "print('PSNR: ' + str(tf.image.psnr(im_input2[6:shape[0]-6,6:shape[0]-6,:], im_input[6:shape[1]-6,6:shape[1]-6,:], max_val=1.0).numpy()))\n",
        "cv2.imwrite('/content/imout.png', im_input2.numpy()*255.0)\n"
      ],
      "metadata": {
        "id": "tV8CgP62d_tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def iterate_files(directory):\n",
        "    for file in os.listdir(directory):\n",
        "        yield os.path.join(directory, file)\n",
        "\n",
        "inpath = '/content/result_images'\n",
        "outpath = '/content/temp_images'\n",
        "!rm /content/temp_images/*\n",
        "os.makedirs(outpath, exist_ok=True)\n",
        "# Iterate over the files of the current directory\n",
        "for file in iterate_files(inpath):\n",
        "    filename = os.path.basename(file)\n",
        "    if filename[:5] == 'Noisy':\n",
        "      filename = 'BLADE_' + filename[6:]\n",
        "      print(filename)\n",
        "      im_input = cv2.imread(inpath + '/' + filename, -1)\n",
        "      print(im_input.shape)\n",
        "      im_out = cv2.bilateralFilter(im_input, 3, 20, 1000)\n",
        "      cv2.imwrite(outpath + '/F0_' + filename[6:], im_out)\n",
        "      cv2.imwrite(outpath + '/N0_' + filename[6:], im_input)\n",
        "\n"
      ],
      "metadata": {
        "id": "rW5YaWSohDQ8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}