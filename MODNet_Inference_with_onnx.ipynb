{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZoliN/colab/blob/main/MODNet_Inference_with_onnx.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjCbspPLvjzh"
      },
      "source": [
        "# MODNet - Inference with onnx\n",
        "\n",
        "This notebook is the modified version of the main [colab demo](https://colab.research.google.com/drive/1GANpbKT06aEFiW-Ssx0DQnnEADcXwQG6?usp=sharing#scrollTo=JOmYOHKfgQ5Y). Refer it for more information. \n",
        "\n",
        "\n",
        "In this demo, we provide a very high performance **inference ready onnx model** for image matting. It also supports dynamic input and output shapes. <b><font color='#00FF0'>The inference time is also very less and you don't need **GPU** to run it. </font></b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVadbSjb6QmV"
      },
      "source": [
        "## 1. Preparation\n",
        "\n",
        "Clone the repository and download the pretrained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXypCYy0q6kG"
      },
      "source": [
        "!pip install onnx onnxruntime\n",
        "import os\n",
        "\n",
        "# clone the repository\n",
        "%cd /content\n",
        "if not os.path.exists('MODNet'):\n",
        "  !git clone https://github.com/manthan3C273/MODNet\n",
        "%cd MODNet/\n",
        "\n",
        "# dowload the onnx model for image matting\n",
        "model = 'modnet.onnx'\n",
        "if not os.path.exists(model):\n",
        "  !gdown --id 1cgycTQlYXpTh26gB9FTnthE7AvruV8hd \\\n",
        "          -O pretrained/modnet.onnx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrr0Jnqd6nyc"
      },
      "source": [
        "## 2. Upload image\n",
        "\n",
        "Upload portrait images to be processed (only PNG and JPG format are supported). \n",
        "\n",
        "Download demo image. Photo by Charlotte May from [Pexels](https://www.pexels.com/photo/unrecognizable-asian-woman-with-rucksack-in-town-5965592/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5CK2gAVq6h_"
      },
      "source": [
        "# dowload image\n",
        "image = 'image.jpg'\n",
        "if not os.path.exists(image):\n",
        "  !gdown --id 1fkyh03NEuSwvjFttYVwV7TjnJML04Xn6 \\\n",
        "          -O image.jpg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUxy6CMD9xLZ"
      },
      "source": [
        "## 3. Inference\n",
        "\n",
        "Run the following command for alpha matte prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1lNsMeS7f5v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7eb02679-f1ca-4751-93f8-e5fe2bd7738b"
      },
      "source": [
        "%cd /content/MODNet\n",
        "!python demo/image_matting/Inference_with_ONNX/inference_onnx.py \\\n",
        "        --image-path=/content/image.jpg \\\n",
        "        --output-path=matte.png \\\n",
        "        --model-path=/content/model-small.onnx"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MODNet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxsim onnxruntime\n",
        "!onnxsim /content/MODNet/pretrained/modnet.onnx /content/MODNet/pretrained/modnet_672.onnx --overwrite-input-shape 1,3,672,512\n"
      ],
      "metadata": {
        "id": "SYb7ZR5C1p-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install onnx-tf\n",
        "!python -m onnxruntime.tools.convert_onnx_models_to_ort --target_platform arm --save_optimized_onnx_model /content/model-small_16.onnx"
      ],
      "metadata": {
        "id": "Ugd83WNJH6Of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install onnx onnxconverter-common\n",
        "import onnx\n",
        "from onnxconverter_common import float16\n",
        "\n",
        "model = onnx.load(\"/content/MODNet/pretrained/modnet.onnx\")\n",
        "model_fp16 = float16.convert_float_to_float16(model, keep_io_types=True)\n",
        "onnx.save(model_fp16, \"/content/MODNet/pretrained/modnet_16.onnx\")"
      ],
      "metadata": {
        "id": "8L5ZVxI7hVR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
        "\n",
        "model_fp32 = '/content/MODNet/pretrained/modnet.onnx'\n",
        "model_quant = '/content/MODNet/pretrained/modnetquant.onnx'\n",
        "quantized_model = quantize_dynamic(model_fp32, model_quant,weight_type=QuantType.QUInt8, optimize_model=False)\n",
        "#quantized_model = quantize_dynamic(model_fp32, model_quant)"
      ],
      "metadata": {
        "id": "KQs3qxDmD0qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVhieG2X_lB4"
      },
      "source": [
        "## 4. Visualization\n",
        "\n",
        "Display the results (from left to right: image, foreground, and alpha matte)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDSVmgi67f4I"
      },
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def combined_display(image, matte):\n",
        "  # calculate display resolution\n",
        "  w, h = image.width, image.height\n",
        "  rw, rh = 800, int(h * 800 / (3 * w))\n",
        "  \n",
        "  # obtain predicted foreground\n",
        "  image = np.asarray(image)\n",
        "  if len(image.shape) == 2:\n",
        "    image = image[:, :, None]\n",
        "  if image.shape[2] == 1:\n",
        "    image = np.repeat(image, 3, axis=2)\n",
        "  elif image.shape[2] == 4:\n",
        "    image = image[:, :, 0:3]\n",
        "  matte = np.repeat(np.asarray(matte)[:, :, None], 3, axis=2) / 255\n",
        "  foreground = image * matte + np.full(image.shape, 255) * (1 - matte)\n",
        "  \n",
        "  # combine image, foreground, and alpha into one line\n",
        "  combined = np.concatenate((image, foreground, matte * 255), axis=1)\n",
        "  combined = Image.fromarray(np.uint8(combined)).resize((rw, rh))\n",
        "  return combined\n",
        "\n",
        "# visualize all images\n",
        "\n",
        "image = Image.open('image.jpg')\n",
        "matte = Image.open('matte.png')\n",
        "display(combined_display(image, matte))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTZfJsweBQ_6"
      },
      "source": [
        "## 5. Download image\n",
        "\n",
        "Image with transparent background will be saved and downloaded. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "ZbN8ZVMtBQeu",
        "outputId": "e236322d-221c-4188-99ac-cf0e2b0a51b9"
      },
      "source": [
        "image.putalpha(matte)\n",
        "image.save('transparent_img.png')\n",
        "\n",
        "from google.colab import files\n",
        "files.download('transparent_img.png')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b8ac7926-867e-451f-b1d8-eff334fc76fd\", \"transparent_img.png\", 16323450)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "#img = img[837:2700, 600:2000,:]\n",
        "#img = img[1237:2700, 600:2000,:]\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import urllib.request\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "url, filename = (\"https://github.com/intel-isl/MiDaS/releases/download/v2/dog.jpg\", \"dog.jpg\")\n",
        "urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "url, filename = (\"https://github.com/intel-isl/MiDaS/releases/download/v2_1/model_opt.tflite\", \"model_opt.tflite\")\n",
        "#urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "# input\n",
        "img = cv2.imread('image.jpg')\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) / 255.0\n",
        "\n",
        "img_resized = tf.image.resize(img, [256,256], method='bicubic', preserve_aspect_ratio=False)\n",
        "#img_resized = tf.transpose(img_resized, [2, 0, 1])\n",
        "img_input = img_resized.numpy()\n",
        "mean=[0.485, 0.456, 0.406]\n",
        "std=[0.229, 0.224, 0.225]\n",
        "img_input = (img_input - mean) / std\n",
        "reshape_img = img_input.reshape(1,256,256,3)\n",
        "tensor = tf.convert_to_tensor(reshape_img, dtype=tf.float32)\n",
        "\n",
        "# load model\n",
        "#interpreter = tf.lite.Interpreter(model_path=\"/content/saved_model/model-small_float32.tflite\")\n",
        "interpreter = tf.lite.Interpreter(model_path=\"/content/saved_model/model_opt_float16.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "input_shape = input_details[0]['shape']\n",
        "\n",
        "# inference\n",
        "interpreter.set_tensor(input_details[0]['index'], tensor)\n",
        "interpreter.invoke()\n",
        "output = interpreter.get_tensor(output_details[0]['index'])\n",
        "output = output.reshape(256, 256)\n",
        "             \n",
        "# output file\n",
        "prediction = cv2.resize(output, (img.shape[1], img.shape[0]), interpolation=cv2.INTER_CUBIC)\n",
        "print(\" Write image to: output.png\")\n",
        "depth_min = prediction.min()\n",
        "depth_max = prediction.max()\n",
        "img_out = (255 * (prediction - depth_min) / (depth_max - depth_min)).astype(\"uint8\")\n",
        "\n",
        "cv2.imwrite(\"output.png\", img_out)\n",
        "plt.imshow(img_out)"
      ],
      "metadata": {
        "id": "IP8EAmIi8hdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from time import time\n",
        "\n",
        "#new_model = tf.keras.models.load_model('out3')\n",
        "model = tf.saved_model.load('outselfie').signatures[\"serving_default\"] \n",
        "\n",
        "\n",
        "inpath = '/content/MODNet/image.jpg'\n",
        "image = tf.io.read_file(inpath)\n",
        "image = tf.image.decode_png(image, dtype=tf.uint8, channels=3)\n",
        "image = image[0000:2000, 500:2000,:]\n",
        "#image = image[1600:2700, 800:1900,:]\n",
        "print(np.max(image))\n",
        "\n",
        "\n",
        "image = (tf.image.resize(images=image, size=[256, 256]))\n",
        "image = tf.cast(image, tf.float32) / 255.0\n",
        "image = tf.expand_dims(image, 0)\n",
        "\n",
        "t1 = time()\n",
        "prediction = model(image)['Identity']\n",
        "t2 = time()\n",
        "elapsed = t2 - t1\n",
        "print('Elapsed time is %f seconds.' % elapsed)\n",
        "\n",
        "bg, prediction = tf.split(prediction, 2, 3)\n",
        "#print(outp)\n",
        "#prediction = tf.where(prediction > bg, 1.0, 0.0)\n",
        "segres = prediction\n",
        "prediction = prediction.numpy()\n",
        "prediction = np.squeeze(prediction)\n",
        "\n",
        "print(\" Write image to: output.png\")\n",
        "depth_min = prediction.min()\n",
        "depth_max = prediction.max()\n",
        "img_out = (255 * (prediction - depth_min) / (depth_max - depth_min)).astype(\"uint8\")\n",
        "\n",
        "cv2.imwrite(\"depth.png\", cv2.cvtColor(prediction*255.0, cv2.COLOR_BGR2RGB) )\n",
        "plt.imshow(img_out)"
      ],
      "metadata": {
        "id": "vUS_j6bM4Dmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive/', force_remount=True)"
      ],
      "metadata": {
        "id": "nUQhdpy36ABZ",
        "outputId": "a3289514-ba78-4810-a279-e3ae62342de9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/isl-org/MiDaS/releases/download/v2_1/model-small.onnx"
      ],
      "metadata": {
        "id": "CoVFHrD38UEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx onnxconverter-common\n",
        "import onnx\n",
        "from onnxconverter_common import float16\n",
        "\n",
        "model = onnx.load(\"/content/model-small.onnx\")\n",
        "model_fp16 = float16.convert_float_to_float16(model, keep_io_types=True)\n",
        "onnx.save(model_fp16, \"/content/model-small_16.onnx\")"
      ],
      "metadata": {
        "id": "G1heEJ8v8oa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!onnx2tf -i /content/model_opt.onnx  -b 1 -cotof -cotoa 1e-1 "
      ],
      "metadata": {
        "id": "qR8v8BhpCLqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U onnx==1.13.1 \\\n",
        "&& pip install -U onnxruntime==1.13.1 \\\n",
        "&& pip install -U onnxsim==0.4.17 \\\n",
        "&& pip install -U onnx2tf \\\n",
        "&& pip install -U h5py==3.7.0 \\\n",
        "&& pip install -U nvidia-pyindex \\\n",
        "&& pip install -U onnx-graphsurgeon \\\n",
        "&& pip install -U  sne4onnx \\\n",
        "&& pip install -U sng4onnx\n",
        "\n",
        "#&& pip install -U simple_onnx_processing_tools \\\n",
        "\n"
      ],
      "metadata": {
        "id": "NoVRDxhSKofC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U  sne4onnx sng4onnx\n",
        "#!pip install -U onnx_tf"
      ],
      "metadata": {
        "id": "wls3_JNLQatQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#img = img[837:2700, 600:2000,:]\n",
        "#img = img[1237:2700, 600:2000,:]\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import urllib.request\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# input\n",
        "img = cv2.imread('/content/image.jpg')\n",
        "img = cv2.resize(img, (512,672), 0, 0, cv2.INTER_AREA)\n",
        "cv2.imwrite(\"ds.png\", img)\n",
        "img = (cv2.cvtColor(img, cv2.COLOR_BGR2RGB) - 127.5) / 127.0\n",
        "\n",
        "#img_resized = tf.image.resize(img, [672,512], method='bilinear', preserve_aspect_ratio=False)\n",
        "#img_resized = tf.transpose(img_resized, [2, 0, 1])\n",
        "#img_input = img_resized.numpy()\n",
        "\n",
        "reshape_img = img.reshape(1,672,512,3)\n",
        "\n",
        "tensor = tf.convert_to_tensor(reshape_img, dtype=tf.float32)\n",
        "\n",
        "# load model\n",
        "interpreter = tf.lite.Interpreter(model_path=\"/content/model.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "input_shape = input_details[0]['shape']\n",
        "\n",
        "# inference\n",
        "interpreter.set_tensor(input_details[0]['index'], tensor)\n",
        "interpreter.invoke()\n",
        "output = interpreter.get_tensor(output_details[0]['index'])\n",
        "output = output.reshape(672, 512)\n",
        "             \n",
        "# output file\n",
        "prediction = output\n",
        "print(\" Write image to: output.png\")\n",
        "img_out = (255 * prediction ).astype(\"uint8\")\n",
        "\n",
        "cv2.imwrite(\"output.png\", img_out)\n",
        "plt.imshow(img_out)"
      ],
      "metadata": {
        "id": "3AZTS5qfaGuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from onnx import checker as ch\n",
        "from onnx import helper as h\n",
        "def save_new_model(opset_version, nodes, graph, out_path, verbose=True):\n",
        "    if verbose:\n",
        "        print(\"Creating new fixed graph...\")\n",
        "    # * create a new graph with new nodes.\n",
        "    new_graph = h.make_graph(\n",
        "        nodes,\n",
        "        graph.name,\n",
        "        graph.input,\n",
        "        graph.output,\n",
        "        initializer=graph.initializer,  # The initializer holds all non-constant weights.\n",
        "    )\n",
        "    if verbose:\n",
        "        print(\"Creating new fixed model...\")\n",
        "    new_model = h.make_model(new_graph, producer_name=\"onnx-fix-nodes\")\n",
        "    new_model.opset_import[0].version = opset_version\n",
        "    ch.check_model(new_model)\n",
        "    if verbose:\n",
        "        print(f\"Saving new model as: {out_path}\")\n",
        "    onnx.save_model(new_model, out_path)\n",
        "\n",
        "\n",
        "def fix_onnx_resize_nodes(model_path: str, out_path: str, verbose=True):\n",
        "    \"\"\"\n",
        "    Method to fix resize nodes giving the following error in Tensorflow\n",
        "    conversions:\n",
        "    - \"Resize coordinate_transformation_mode=pytorch_half_pixel is not supported in Tensorflow\"\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(f\"Loading Model: {model_path}\")\n",
        "    # * load model.\n",
        "    model = onnx.load_model(model_path)\n",
        "    ch.check_model(model)\n",
        "    # * get model opset version.\n",
        "    opset_version = model.opset_import[0].version\n",
        "    graph = model.graph\n",
        "\n",
        "    new_nodes = []\n",
        "    if verbose:\n",
        "        print(\"Fixing Resize nodes...\")\n",
        "    for i, node in enumerate(graph.node):\n",
        "        if node.op_type == \"Resize\":\n",
        "            print(node)\n",
        "            new_resize = onnx.helper.make_node(\n",
        "                'Resize',\n",
        "                inputs=node.input,\n",
        "                outputs=node.output,\n",
        "                name=node.name,\n",
        "                coordinate_transformation_mode='half_pixel',  # Instead of pytorch_half_pixel, unsupported by Tensorflow\n",
        "                mode='linear',\n",
        "            )\n",
        "            # Update node\n",
        "            new_nodes += [new_resize]\n",
        "        else:\n",
        "            new_nodes += [node]\n",
        "\n",
        "    save_new_model(opset_version=opset_version, graph=graph, nodes=new_nodes,\n",
        "                   out_path=out_path, verbose=verbose)"
      ],
      "metadata": {
        "id": "6g8Ik-2fkVHg"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fix_onnx_resize_nodes('/content/model_opt.onnx', '/content/model_optfixed.onnx')"
      ],
      "metadata": {
        "id": "z3RN8Ockka3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tflite2onnx"
      ],
      "metadata": {
        "id": "GQjY3Yygl0j9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tflite2onnx\n",
        "\n",
        "tflite_path = '/content/model_opt.tflite'\n",
        "onnx_path = '/content/model_opt.onnx'\n",
        "\n",
        "tflite2onnx.convert(tflite_path, onnx_path)"
      ],
      "metadata": {
        "id": "VvdyLgJk28us"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}