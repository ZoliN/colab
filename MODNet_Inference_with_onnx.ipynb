{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZoliN/colab/blob/main/MODNet_Inference_with_onnx.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjCbspPLvjzh"
      },
      "source": [
        "# MODNet - Inference with onnx\n",
        "\n",
        "This notebook is the modified version of the main [colab demo](https://colab.research.google.com/drive/1GANpbKT06aEFiW-Ssx0DQnnEADcXwQG6?usp=sharing#scrollTo=JOmYOHKfgQ5Y). Refer it for more information. \n",
        "\n",
        "\n",
        "In this demo, we provide a very high performance **inference ready onnx model** for image matting. It also supports dynamic input and output shapes. <b><font color='#00FF0'>The inference time is also very less and you don't need **GPU** to run it. </font></b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVadbSjb6QmV"
      },
      "source": [
        "## 1. Preparation\n",
        "\n",
        "Clone the repository and download the pretrained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXypCYy0q6kG"
      },
      "source": [
        "!pip install onnx onnxruntime\n",
        "import os\n",
        "\n",
        "# clone the repository\n",
        "%cd /content\n",
        "if not os.path.exists('MODNet'):\n",
        "  !git clone https://github.com/manthan3C273/MODNet\n",
        "%cd MODNet/\n",
        "\n",
        "# dowload the onnx model for image matting\n",
        "model = 'modnet.onnx'\n",
        "if not os.path.exists(model):\n",
        "  !gdown --id 1cgycTQlYXpTh26gB9FTnthE7AvruV8hd \\\n",
        "          -O pretrained/modnet.onnx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrr0Jnqd6nyc"
      },
      "source": [
        "## 2. Upload image\n",
        "\n",
        "Upload portrait images to be processed (only PNG and JPG format are supported). \n",
        "\n",
        "Download demo image. Photo by Charlotte May from [Pexels](https://www.pexels.com/photo/unrecognizable-asian-woman-with-rucksack-in-town-5965592/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5CK2gAVq6h_"
      },
      "source": [
        "# dowload image\n",
        "image = 'image.jpg'\n",
        "if not os.path.exists(image):\n",
        "  !gdown --id 1fkyh03NEuSwvjFttYVwV7TjnJML04Xn6 \\\n",
        "          -O image.jpg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUxy6CMD9xLZ"
      },
      "source": [
        "## 3. Inference\n",
        "\n",
        "Run the following command for alpha matte prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1lNsMeS7f5v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cbc7eb0-0b4c-4e46-d538-d2d4d4a69a3f"
      },
      "source": [
        "%cd /content/MODNet\n",
        "!python demo/image_matting/Inference_with_ONNX/inference_onnx.py \\\n",
        "        --image-path=/content/MODNet/image.jpg \\\n",
        "        --output-path=matte.png \\\n",
        "        --model-path=/content/MODNet/pretrained/modnet.onnx"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MODNet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxsim onnxruntime\n",
        "!onnxsim /content/MODNet/pretrained/modnet.onnx /content/MODNet/pretrained/modnet_672.onnx --overwrite-input-shape 1,3,672,512\n"
      ],
      "metadata": {
        "id": "SYb7ZR5C1p-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install onnx-tf\n",
        "!python -m onnxruntime.tools.convert_onnx_models_to_ort --target_platform arm --save_optimized_onnx_model /content/model-small_16.onnx"
      ],
      "metadata": {
        "id": "Ugd83WNJH6Of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install onnx onnxconverter-common\n",
        "import onnx\n",
        "from onnxconverter_common import float16\n",
        "\n",
        "model = onnx.load(\"/content/MODNet/pretrained/modnet.onnx\")\n",
        "model_fp16 = float16.convert_float_to_float16(model, keep_io_types=True)\n",
        "onnx.save(model_fp16, \"/content/MODNet/pretrained/modnet_16.onnx\")"
      ],
      "metadata": {
        "id": "8L5ZVxI7hVR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
        "\n",
        "model_fp32 = '/content/MODNet/pretrained/modnet.onnx'\n",
        "model_quant = '/content/MODNet/pretrained/modnetquant.onnx'\n",
        "quantized_model = quantize_dynamic(model_fp32, model_quant,weight_type=QuantType.QUInt8, optimize_model=False)\n",
        "#quantized_model = quantize_dynamic(model_fp32, model_quant)"
      ],
      "metadata": {
        "id": "KQs3qxDmD0qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVhieG2X_lB4"
      },
      "source": [
        "## 4. Visualization\n",
        "\n",
        "Display the results (from left to right: image, foreground, and alpha matte)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDSVmgi67f4I"
      },
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def combined_display(image, matte):\n",
        "  # calculate display resolution\n",
        "  w, h = image.width, image.height\n",
        "  rw, rh = 800, int(h * 800 / (3 * w))\n",
        "  \n",
        "  # obtain predicted foreground\n",
        "  image = np.asarray(image)\n",
        "  if len(image.shape) == 2:\n",
        "    image = image[:, :, None]\n",
        "  if image.shape[2] == 1:\n",
        "    image = np.repeat(image, 3, axis=2)\n",
        "  elif image.shape[2] == 4:\n",
        "    image = image[:, :, 0:3]\n",
        "  matte = np.repeat(np.asarray(matte)[:, :, None], 3, axis=2) / 255\n",
        "  foreground = image * matte + np.full(image.shape, 255) * (1 - matte)\n",
        "  \n",
        "  # combine image, foreground, and alpha into one line\n",
        "  combined = np.concatenate((image, foreground, matte * 255), axis=1)\n",
        "  combined = Image.fromarray(np.uint8(combined)).resize((rw, rh))\n",
        "  return combined\n",
        "\n",
        "# visualize all images\n",
        "\n",
        "image = Image.open('image.jpg')\n",
        "matte = Image.open('matte.png')\n",
        "display(combined_display(image, matte))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTZfJsweBQ_6"
      },
      "source": [
        "## 5. Download image\n",
        "\n",
        "Image with transparent background will be saved and downloaded. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "ZbN8ZVMtBQeu",
        "outputId": "e236322d-221c-4188-99ac-cf0e2b0a51b9"
      },
      "source": [
        "image.putalpha(matte)\n",
        "image.save('transparent_img.png')\n",
        "\n",
        "from google.colab import files\n",
        "files.download('transparent_img.png')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b8ac7926-867e-451f-b1d8-eff334fc76fd\", \"transparent_img.png\", 16323450)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "#img = img[837:2700, 600:2000,:]\n",
        "#img = img[1237:2700, 600:2000,:]\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import urllib.request\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "url, filename = (\"https://github.com/intel-isl/MiDaS/releases/download/v2/dog.jpg\", \"dog.jpg\")\n",
        "urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "url, filename = (\"https://github.com/intel-isl/MiDaS/releases/download/v2_1/model_opt.tflite\", \"model_opt.tflite\")\n",
        "urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "# input\n",
        "img = cv2.imread('image.jpg')\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) / 255.0\n",
        "\n",
        "img_resized = tf.image.resize(img, [256,256], method='bicubic', preserve_aspect_ratio=False)\n",
        "#img_resized = tf.transpose(img_resized, [2, 0, 1])\n",
        "img_input = img_resized.numpy()\n",
        "mean=[0.485, 0.456, 0.406]\n",
        "std=[0.229, 0.224, 0.225]\n",
        "img_input = (img_input - mean) / std\n",
        "reshape_img = img_input.reshape(1,256,256,3)\n",
        "tensor = tf.convert_to_tensor(reshape_img, dtype=tf.float32)\n",
        "\n",
        "# load model\n",
        "#interpreter = tf.lite.Interpreter(model_path=\"/content/saved_model/model-small_float32.tflite\")\n",
        "interpreter = tf.lite.Interpreter(model_path=\"/content/saved_model/model_opt_float16.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "input_shape = input_details[0]['shape']\n",
        "\n",
        "# inference\n",
        "interpreter.set_tensor(input_details[0]['index'], tensor)\n",
        "interpreter.invoke()\n",
        "output = interpreter.get_tensor(output_details[0]['index'])\n",
        "output = output.reshape(256, 256)\n",
        "             \n",
        "# output file\n",
        "prediction = cv2.resize(output, (img.shape[1], img.shape[0]), interpolation=cv2.INTER_CUBIC)\n",
        "print(\" Write image to: output.png\")\n",
        "depth_min = prediction.min()\n",
        "depth_max = prediction.max()\n",
        "img_out = (255 * (prediction - depth_min) / (depth_max - depth_min)).astype(\"uint8\")\n",
        "\n",
        "cv2.imwrite(\"output.png\", img_out)\n",
        "plt.imshow(img_out)"
      ],
      "metadata": {
        "id": "IP8EAmIi8hdV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "00f6a615-4699-407b-84fd-92f66963b72e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-b5f438105cb3>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# load model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m#interpreter = tf.lite.Interpreter(model_path=\"/content/saved_model/model-small_float32.tflite\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0minterpreter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInterpreter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/saved_model/model_opt_float16.tflite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallocate_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0minput_details\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_input_details\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/interpreter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_path, model_content, experimental_delegates, num_threads, experimental_op_resolver_type, experimental_preserve_all_tensors)\u001b[0m\n\u001b[1;32m    447\u001b[0m       ]\n\u001b[1;32m    448\u001b[0m       self._interpreter = (\n\u001b[0;32m--> 449\u001b[0;31m           _interpreter_wrapper.CreateWrapperFromFile(\n\u001b[0m\u001b[1;32m    450\u001b[0m               \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_resolver_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_op_registerers_by_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m               custom_op_registerers_by_func, experimental_preserve_all_tensors))\n",
            "\u001b[0;31mValueError\u001b[0m: Could not open '/content/saved_model/model_opt_float16.tflite'."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from time import time\n",
        "\n",
        "#new_model = tf.keras.models.load_model('out3')\n",
        "model = tf.saved_model.load('outselfie').signatures[\"serving_default\"] \n",
        "\n",
        "\n",
        "inpath = '/content/MODNet/image.jpg'\n",
        "image = tf.io.read_file(inpath)\n",
        "image = tf.image.decode_png(image, dtype=tf.uint8, channels=3)\n",
        "image = image[0000:2000, 500:2000,:]\n",
        "#image = image[1600:2700, 800:1900,:]\n",
        "print(np.max(image))\n",
        "\n",
        "\n",
        "image = (tf.image.resize(images=image, size=[256, 256]))\n",
        "image = tf.cast(image, tf.float32) / 255.0\n",
        "image = tf.expand_dims(image, 0)\n",
        "\n",
        "t1 = time()\n",
        "prediction = model(image)['Identity']\n",
        "t2 = time()\n",
        "elapsed = t2 - t1\n",
        "print('Elapsed time is %f seconds.' % elapsed)\n",
        "\n",
        "bg, prediction = tf.split(prediction, 2, 3)\n",
        "#print(outp)\n",
        "#prediction = tf.where(prediction > bg, 1.0, 0.0)\n",
        "segres = prediction\n",
        "prediction = prediction.numpy()\n",
        "prediction = np.squeeze(prediction)\n",
        "\n",
        "print(\" Write image to: output.png\")\n",
        "depth_min = prediction.min()\n",
        "depth_max = prediction.max()\n",
        "img_out = (255 * (prediction - depth_min) / (depth_max - depth_min)).astype(\"uint8\")\n",
        "\n",
        "cv2.imwrite(\"depth.png\", cv2.cvtColor(prediction*255.0, cv2.COLOR_BGR2RGB) )\n",
        "plt.imshow(img_out)"
      ],
      "metadata": {
        "id": "vUS_j6bM4Dmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive/', force_remount=True)"
      ],
      "metadata": {
        "id": "nUQhdpy36ABZ",
        "outputId": "40a05e91-ce7e-4c97-fbea-e1b2b4314e06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/isl-org/MiDaS/releases/download/v2_1/model-small.onnx"
      ],
      "metadata": {
        "id": "CoVFHrD38UEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx onnxconverter-common\n",
        "import onnx\n",
        "from onnxconverter_common import float16\n",
        "\n",
        "model = onnx.load(\"/content/model-small.onnx\")\n",
        "model_fp16 = float16.convert_float_to_float16(model, keep_io_types=True)\n",
        "onnx.save(model_fp16, \"/content/model-small_16.onnx\")"
      ],
      "metadata": {
        "id": "G1heEJ8v8oa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!onnx2tf -i /content/model_opt.onnx  -b 1 -cotof -cotoa 1e-1 "
      ],
      "metadata": {
        "id": "qR8v8BhpCLqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U onnx==1.13.1 \\\n",
        "&& pip install -U onnxruntime==1.13.1 \\\n",
        "&& pip install -U onnxsim==0.4.17 \\\n",
        "&& pip install -U onnx2tf \\\n",
        "&& pip install -U h5py==3.7.0 \\\n",
        "&& pip install -U nvidia-pyindex \\\n",
        "&& pip install -U onnx-graphsurgeon \\\n",
        "&& pip install -U  sne4onnx \\\n",
        "&& pip install -U sng4onnx\n",
        "\n",
        "#&& pip install -U simple_onnx_processing_tools \\\n",
        "\n"
      ],
      "metadata": {
        "id": "NoVRDxhSKofC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U  sne4onnx sng4onnx\n",
        "#!pip install -U onnx_tf"
      ],
      "metadata": {
        "id": "wls3_JNLQatQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#img = img[837:2700, 600:2000,:]\n",
        "#img = img[1237:2700, 600:2000,:]\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import urllib.request\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# input\n",
        "img = cv2.imread('/content/image.jpg')\n",
        "img = cv2.resize(img, (512,672), 0, 0, cv2.INTER_AREA)\n",
        "cv2.imwrite(\"ds.png\", img)\n",
        "img = (cv2.cvtColor(img, cv2.COLOR_BGR2RGB) - 127.5) / 127.0\n",
        "\n",
        "#img_resized = tf.image.resize(img, [672,512], method='bilinear', preserve_aspect_ratio=False)\n",
        "#img_resized = tf.transpose(img_resized, [2, 0, 1])\n",
        "#img_input = img_resized.numpy()\n",
        "\n",
        "reshape_img = img.reshape(1,672,512,3)\n",
        "\n",
        "tensor = tf.convert_to_tensor(reshape_img, dtype=tf.float32)\n",
        "\n",
        "# load model\n",
        "interpreter = tf.lite.Interpreter(model_path=\"/content/model.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "input_shape = input_details[0]['shape']\n",
        "\n",
        "# inference\n",
        "interpreter.set_tensor(input_details[0]['index'], tensor)\n",
        "interpreter.invoke()\n",
        "output = interpreter.get_tensor(output_details[0]['index'])\n",
        "output = output.reshape(672, 512)\n",
        "             \n",
        "# output file\n",
        "prediction = output\n",
        "print(\" Write image to: output.png\")\n",
        "img_out = (255 * prediction ).astype(\"uint8\")\n",
        "\n",
        "cv2.imwrite(\"output.png\", img_out)\n",
        "plt.imshow(img_out)"
      ],
      "metadata": {
        "id": "3AZTS5qfaGuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from onnx import checker as ch\n",
        "from onnx import helper as h\n",
        "def save_new_model(opset_version, nodes, graph, out_path, verbose=True):\n",
        "    if verbose:\n",
        "        print(\"Creating new fixed graph...\")\n",
        "    # * create a new graph with new nodes.\n",
        "    new_graph = h.make_graph(\n",
        "        nodes,\n",
        "        graph.name,\n",
        "        graph.input,\n",
        "        graph.output,\n",
        "        initializer=graph.initializer,  # The initializer holds all non-constant weights.\n",
        "    )\n",
        "    if verbose:\n",
        "        print(\"Creating new fixed model...\")\n",
        "    new_model = h.make_model(new_graph, producer_name=\"onnx-fix-nodes\")\n",
        "    new_model.opset_import[0].version = opset_version\n",
        "    ch.check_model(new_model)\n",
        "    if verbose:\n",
        "        print(f\"Saving new model as: {out_path}\")\n",
        "    onnx.save_model(new_model, out_path)\n",
        "\n",
        "\n",
        "def fix_onnx_resize_nodes(model_path: str, out_path: str, verbose=True):\n",
        "    \"\"\"\n",
        "    Method to fix resize nodes giving the following error in Tensorflow\n",
        "    conversions:\n",
        "    - \"Resize coordinate_transformation_mode=pytorch_half_pixel is not supported in Tensorflow\"\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(f\"Loading Model: {model_path}\")\n",
        "    # * load model.\n",
        "    model = onnx.load_model(model_path)\n",
        "    ch.check_model(model)\n",
        "    # * get model opset version.\n",
        "    opset_version = model.opset_import[0].version\n",
        "    graph = model.graph\n",
        "\n",
        "    new_nodes = []\n",
        "    if verbose:\n",
        "        print(\"Fixing Resize nodes...\")\n",
        "    for i, node in enumerate(graph.node):\n",
        "        if node.op_type == \"Resize\":\n",
        "            print(node)\n",
        "            new_resize = onnx.helper.make_node(\n",
        "                'Resize',\n",
        "                inputs=node.input,\n",
        "                outputs=node.output,\n",
        "                name=node.name,\n",
        "                coordinate_transformation_mode='half_pixel',  # Instead of pytorch_half_pixel, unsupported by Tensorflow\n",
        "                mode='linear',\n",
        "            )\n",
        "            # Update node\n",
        "            new_nodes += [new_resize]\n",
        "        else:\n",
        "            new_nodes += [node]\n",
        "\n",
        "    save_new_model(opset_version=opset_version, graph=graph, nodes=new_nodes,\n",
        "                   out_path=out_path, verbose=verbose)"
      ],
      "metadata": {
        "id": "6g8Ik-2fkVHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fix_onnx_resize_nodes('/content/model_opt.onnx', '/content/model_optfixed.onnx')"
      ],
      "metadata": {
        "id": "z3RN8Ockka3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tflite2onnx"
      ],
      "metadata": {
        "id": "GQjY3Yygl0j9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tflite2onnx\n",
        "\n",
        "tflite_path = '/content/model_opt.tflite'\n",
        "onnx_path = '/content/model_opt.onnx'\n",
        "\n",
        "tflite2onnx.convert(tflite_path, onnx_path)"
      ],
      "metadata": {
        "id": "VvdyLgJk28us",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "968f5b19-af59-4a7c-af9e-c9e309a8e468"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tflite2onnx:Empty tensor used, please double confirm your code path!\n",
            "WARNING:tflite2onnx:Empty tensor used, please double confirm your code path!\n",
            "WARNING:tflite2onnx:Empty tensor used, please double confirm your code path!\n",
            "WARNING:tflite2onnx:Empty tensor used, please double confirm your code path!\n",
            "WARNING:tflite2onnx:Empty tensor used, please double confirm your code path!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!cp -r /content/gdrive/MyDrive/hdrnet/hdrnet /content/\n",
        "%cd /content/hdrnet\n",
        "import hdrnet.guided_filter as gf\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import numpy as np\n",
        "from importlib import reload\n",
        "reload(gf)\n",
        "\n",
        "\n",
        "# depth = tf.io.read_file(\"/content/depth.png\")\n",
        "# depth = tf.image.decode_png(depth, dtype=tf.uint8, channels=1)\n",
        "#matte = tf.io.read_file(\"/content/matte.png\")\n",
        "#matte = tf.image.decode_png(matte, dtype=tf.uint8, channels=1)\n",
        "dir = 'im2'\n",
        "\n",
        "imgc = cv2.imread('/content/' + dir +  '/image.jpg')\n",
        "img = cv2.cvtColor(imgc, cv2.COLOR_BGR2GRAY)\n",
        "img = img.astype('float') / 255.0\n",
        "rows = np.shape(img)[0]\n",
        "cols = np.shape(img)[1]\n",
        "\n",
        "img4 = cv2.resize(img, (cols//4,rows//4), 0, 0, cv2.INTER_LINEAR)\n",
        "\n",
        "\n",
        "matte1 = cv2.imread('/content/' + dir +  '/matte.png')\n",
        "matte1 = np.split(matte1,3,2)[0].astype('float') / 255.0\n",
        "erodedMatte = cv2.erode(matte1, np.ones((3, 3), 'uint8'))\n",
        "erodedMatte = cv2.resize(erodedMatte, (cols,rows), 0, 0, cv2.INTER_LINEAR)\n",
        "\n",
        "matte4 = cv2.resize(matte1, (cols//4,rows//4), 0, 0, cv2.INTER_LINEAR)\n",
        "matte = cv2.resize(matte1, (cols,rows), 0, 0, cv2.INTER_LINEAR)\n",
        "cv2.imwrite(\"/content/mattehr.png\", (np.clip(matte * 255.0, 0.,255.) ).astype(\"uint8\") )\n",
        "\n",
        "w = np.where(matte4 > 0.3, 0.0, 1.0 - matte4) \n",
        "wimg = img4 * w\n",
        "wimgw = np.dstack((wimg, w))\n",
        "blurred = cv2.blur(wimgw, (5,5))\n",
        "imb, wb = np.split(blurred,2,2)\n",
        "meanim = np.where(wb >= 0.01, imb / wb, 0.0) \n",
        "\n",
        "wimg = img4 * img4 * w\n",
        "wimgw = np.dstack((wimg, w))\n",
        "blurred = cv2.blur(wimgw, (5,5))\n",
        "imb, wb = np.split(blurred,2,2)\n",
        "mean2im = np.where(wb >= 0.01, imb / wb, 0.0) \n",
        "\n",
        "varim = mean2im - meanim * meanim\n",
        "varim = cv2.dilate(varim, np.ones((3, 3), 'uint8'))\n",
        "#varim = cv2.dilate(varim, np.ones((3, 3), 'uint8'))\n",
        "\n",
        "\n",
        "varim = tf.squeeze(varim)\n",
        "var_min = 0\n",
        "var_max = 0.07\n",
        "varim = np.clip((255 * (varim - var_min) / (var_max - var_min)), 0.,255.).astype(\"uint8\")\n",
        "cv2.imwrite('/content/var1.png', varim.astype(\"uint8\") )\n",
        "\n",
        "mitigateFactor = 1.0 + 0.3 * np.clip(((varim - var_min) / (var_max - var_min)), 0.,1.)\n",
        "matte4 = np.power(matte4, mitigateFactor)\n",
        "#matte = cv2.resize(matte4, (cols,rows), 0, 0, cv2.INTER_LINEAR)\n",
        "cv2.imwrite(\"/content/mattehr2.png\", (np.clip(matte * 255.0, 0.,255.) ).astype(\"uint8\") )\n",
        "\n",
        "\n",
        "#img = cv2.resize(img, (512,672), 0, 0, cv2.INTER_AREA)\n",
        "#matte = tf.image.resize(images=matte, size=[4080, 3072])\n",
        "\n",
        "erodedMatte = tf.cast(erodedMatte, tf.float32) \n",
        "\n",
        "matte = tf.cast(matte, tf.float32) \n",
        "img = tf.cast(img, tf.float32) \n",
        "\n",
        "# depth = tf.cast(depth, tf.float32) / 255.0\n",
        "# dilatedDepth = cv2.erode(depth.numpy(), np.ones((3, 3), 'uint8'))\n",
        "\n",
        "# depth = tf.expand_dims(depth, 0)\n",
        "img = tf.expand_dims(img, 0)\n",
        "img = tf.expand_dims(img, 3)\n",
        "matte = tf.expand_dims(matte, 0)\n",
        "matte = tf.expand_dims(matte, 3)\n",
        "erodedMatte = tf.expand_dims(erodedMatte, 0)\n",
        "erodedMatte = tf.expand_dims(erodedMatte, 3)\n",
        "# dilatedDepth = tf.expand_dims(dilatedDepth, 0)\n",
        "# dilatedDepth = tf.expand_dims(dilatedDepth, 3)\n",
        "print(tf.shape(img))\n",
        "print(tf.shape(matte))\n",
        "print(tf.shape(erodedMatte))\n",
        "\n",
        "# depth = tf.image.resize(images=depth, size=[672, 512])\n",
        "# dilatedDepth = tf.image.resize(images=dilatedDepth, size=[672, 512])\n",
        "# cv2.imwrite(\"/content/depthout.png\", (255 * tf.squeeze(depth).numpy() ).astype(\"uint8\") )\n",
        "\n",
        "# depthlr = tf.image.resize(images=depth, size=[672//2, 512//2], method='nearest')\n",
        "fgdiv=2\n",
        "imglr = tf.image.resize(images=img, size=[rows//fgdiv, cols//fgdiv], method='nearest')\n",
        "mattelr = tf.image.resize(images=matte, size=[rows//fgdiv, cols//fgdiv], method='nearest')\n",
        "erodedMattelr = tf.image.resize(images=erodedMatte, size=[rows//fgdiv, cols//fgdiv], method='nearest')\n",
        "# dilatedDepthlr = tf.image.resize(images=dilatedDepth, size=[672//2, 512//2], method='nearest')\n",
        "\n",
        "#img = gf.fast_guided_filter(mattelr, depthlr, matte, depthlr, 2, eps=1e-8, nhwc=True)\n",
        "img = gf.fast_guided_filter(imglr, mattelr, img, erodedMattelr, 2, eps=0.01*0.01, nhwc=True)\n",
        "\n",
        "img = tf.squeeze(img).numpy()\n",
        "mitigateFactor = cv2.resize(mitigateFactor, (cols,rows), 0, 0, cv2.INTER_LINEAR)\n",
        "img = np.power(img, mitigateFactor)\n",
        "\n",
        "\n",
        "cv2.imwrite(\"/content/out.png\", (np.clip(255 * img, 0.,255.) ).astype(\"uint8\") )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMTcp0wFGWjj",
        "outputId": "4dd70fc8-9d99-4b41-9a35-e3adfa8f75bf"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/hdrnet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-40-d16de922dce0>:40: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  meanim = np.where(wb >= 0.01, imb / wb, 0.0)\n",
            "<ipython-input-40-d16de922dce0>:46: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  mean2im = np.where(wb >= 0.01, imb / wb, 0.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([   1 4080 3072    1], shape=(4,), dtype=int32)\n",
            "tf.Tensor([   1 4080 3072    1], shape=(4,), dtype=int32)\n",
            "tf.Tensor([   1 4080 3072    1], shape=(4,), dtype=int32)\n",
            "-1.3294938710927964\n",
            "-0.10434166834904178\n",
            "-0.010781444892287254\n",
            "-0.0003669216241776942\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-40-d16de922dce0>:105: RuntimeWarning: invalid value encountered in power\n",
            "  img = np.power(img, mitigateFactor)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import hdrnet.guided_filter as gf\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import numpy as np\n",
        "from importlib import reload\n",
        "reload(gf)\n",
        "\n",
        "\n",
        "img = cv2.imread('/content/image.jpg')\n",
        "#img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "img = cv2.resize(img, (512,672), 0, 0, cv2.INTER_AREA)\n",
        "\n",
        "matte = cv2.imread('/content/matte.png')\n",
        "\n",
        "matteEroded = cv2.erode(matte, np.ones((3, 3), 'uint8'))\n",
        "\n",
        "img = img.astype(\"float\") / 255.0\n",
        "matte = matte.astype(\"float\") / 255.0\n",
        "print(np.shape(matte))\n",
        "\n",
        "matteThres = np.where(matte > 0.1, 1.0, 0.0)\n",
        "\n",
        "matteEroded = cv2.erode(matteThres, np.ones((3, 3), 'uint8'))\n",
        "matteEroded = cv2.erode(matteEroded, np.ones((3, 3), 'uint8'))\n",
        "matteBorder = matteThres - matteEroded\n",
        "\n",
        "imgM = img * matte\n",
        "imgMT = img * matteThres\n",
        "img2 = img * matteEroded\n",
        "img2 = cv2.dilate(img2, np.ones((3, 3), 'uint8'))\n",
        "img2 = cv2.dilate(img2, np.ones((3, 3), 'uint8'))\n",
        "\n",
        "mult = 1.0 / (1.0 + np.sum((img - img2) ** 2, axis=2, keepdims = True)*20.)\n",
        "mult*=(np.split(matteThres,3,2)[0] - np.split(matteEroded,3,2)[0])\n",
        "\n",
        "matte = matte * np.where(mult == 0.0, 1.0, mult)\n",
        "imgM2 = img * matte\n",
        "\n",
        "matte = np.split(matte,3,2)[0]\n",
        "\n",
        "cv2.imwrite(\"/content/outM.png\", (np.clip(255 * tf.squeeze(imgM), 0.,255.) ).astype(\"uint8\") )\n",
        "cv2.imwrite(\"/content/out2.png\", (np.clip(255 * tf.squeeze(img2), 0.,255.) ).astype(\"uint8\") )\n",
        "cv2.imwrite(\"/content/outMT.png\", (np.clip(255 * tf.squeeze(imgMT), 0.,255.) ).astype(\"uint8\") )\n",
        "cv2.imwrite(\"/content/out2.png\", (np.clip(255 * tf.squeeze(img2), 0.,255.) ).astype(\"uint8\") )\n",
        "cv2.imwrite(\"/content/mult.png\", (np.clip(255 * tf.squeeze(mult), 0.,255.) ).astype(\"uint8\") )\n",
        "cv2.imwrite(\"/content/mattemod.png\", (np.clip(255 * tf.squeeze(matte), 0.,255.) ).astype(\"uint8\") )\n",
        "cv2.imwrite(\"/content/outM2.png\", (np.clip(255 * tf.squeeze(imgM2), 0.,255.) ).astype(\"uint8\") )\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bfeGH70XR_Z",
        "outputId": "0c288a68-752c-4d8d-bb98-499f61478cab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(672, 512, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    }
  ]
}